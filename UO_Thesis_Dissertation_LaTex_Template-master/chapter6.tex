% Chapter 6 ---------------------------------
\chapter{Conclusion} \label{conclusion}
In this thesis, we have proposed a framework for suggesting high-performance scientific toolkits catered to specific problems and experimental setup by using a model-based approach. On the way to guarantee both improved performances and reliability, we have explored a variety of models. Some models are not as of yet reliable, but we have analyzed and discussed them in this thesis nonetheless. In specific, predictive modeling has been a challenge due to various constraints in available data. 
 
 But from our proposed results, we can also say that our framework is reliable in classification. It can classify if a specific experimental setup for processing a graph dataset in parallel is good or not with \textbf{97\% accuracy}. We can also say that selecting the experiments which are predicted to be good by the framework provides an average of 200\% improvement in performance across four algorithms.    
 
 Similarly, we can also say that our framework is reliable in ranking experiments. In the previous chapter, we established that from our ranked list of solvers for over 1,500 different test sets across two separate learning sets for parallel and serial execution, \textbf{the \#1 ranked solver is never a bad solver}. Also, the \textbf{the \#1 ranked solver is always better than the baseline solver}, which is the default solver in PETSc. Based on the speedups for serial execution with MOOSE learning set, we can say that the \#1 ranked solvers are on average seven times better than the baseline with a few solvers peaking at 800-fold improvement. Likewise, we can say that for parallel execution with the SuiteSparse learning set, the \#1 ranked solvers are 700 times better than the baseline with a few solvers extraordinarily peaking with 90,000 times improvement in performance.
 
 To summarize, we have provided reliable solutions for two of the three objectives set out for the framework, thereby making it a much better alternative for choosing HPC toolkits than choosing it based on empirical results from running time-consuming experiments. 
