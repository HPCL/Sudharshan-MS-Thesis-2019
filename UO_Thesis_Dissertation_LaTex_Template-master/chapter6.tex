% Chapter 6 ---------------------------------
\chapter{Conclusion} \label{matrixfree}
In this thesis, we have proposed a framework for suggesting scientific toolkits catered to specific problems and experimental setups by using a model-based approach. On the way to guarantee both improved performances and reliability, we have explored a variety of models. Some models aren't as of yet reliable, but we have analyzed and discussed them in this thesis nonetheless. In specific, predictive modeling has been a challenge due to various constraints in available data. 
 
 But from our proposed results, we can also say for sure that our framework is reliable in classification. It can classify if a specific experimental setup for processing a graph dataset in parallel is good or not with \textbf{97\% accuracy}. We can also say that selecting the experiments which are predicted to be good by the framework provides an average of 200\% improvement in performance across four algorithms.\todo{sound a little sketchy though since it ranges from 700-5 \%}    
 
 Similarly, we can also say that our framework is reliable in ranking experiments. In the previous chapter, we established that from our ranked list of solvers for over 1500 different test sets across two separate learning sets for parallel and serial execution, \textbf{the \#1 ranked solver is never a bad solver}. Also, the \textbf{the \#1 ranked solver is always better than the baseline solver}, which is the default solver offered PETSc. Based on the speedups for serial execution with MOOSE learning set, we can say that the \#1 ranked solvers are 7 times better than the baseline with a few solvers peaking at 800 times improvement. Likewise, we can say that for parallel execution with SuiteSparse learning set, the \#1 ranked solvers are 700 times better than the baseline with a few solvers extraordinarily peaking with 90,000 times improvement in performance.
 
 To summarize, we have provided reliable solutions for two of the three objectives set out for the framework, thereby making it a much better alternative for choosing scientific toolkits than choosing it based on empirical results from running time-consuming experiments. 
